{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding Decision Trees\n",
    "What is a Decision Tree?\n",
    "A Decision Tree is a type of supervised machine learning algorithm used for both classification and regression tasks. It works like a flowchart structure where each internal node represents a decision based on a feature, each branch represents an outcome of the decision, and each leaf node represents a final prediction or result. Decision Trees model decisions and their possible consequences by learning simple decision rules inferred from the data features."
   ],
   "id": "562d2a7cbd9c132c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why Use Decision Trees?\n",
    "Easy to Understand and Interpret: The model's flowchart-like structure makes it easy to visualize and understand, even for people without a strong background in machine learning.\n",
    "No Need for Feature Scaling: Unlike algorithms such as SVM or KNN, Decision Trees do not require normalization or scaling of data.\n",
    "Handles Both Types of Data: They can work with both numerical and categorical data.\n",
    "Minimal Data Preparation: Requires less data cleaning compared to other algorithms. Missing values and outliers often do not significantly impact their performance.\n",
    "Real-World Applications\n",
    "Medical Diagnosis: Used to predict diseases or health conditions based on patient data.\n",
    "Credit Risk Analysis: Helps financial institutions decide whether to approve or deny loans based on an applicant's credit history.\n",
    "Customer Segmentation: Used in marketing to classify customer behavior and target specific groups.\n",
    "Fraud Detection: Detects fraudulent activities by analyzing patterns in transactional data.\n",
    "Recommendation Systems: Suggests products or services based on user preferences and historical data."
   ],
   "id": "5c4156b3318a0006"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Anatomy of a Decision Tree:\n",
    "\n",
    "Root Node\n",
    "The topmost node in a decision tree. It represents the entire dataset, which is then split into subsets based on a specific feature. The root node acts as the starting point for making decisions.\n",
    "\n",
    "Internal Nodes (Decision Nodes)\n",
    "These are nodes that represent tests or decisions on an attribute. Based on the outcomes of these decisions, the tree splits further into branches.\n",
    "\n",
    "Leaf Nodes (Terminal Nodes)\n",
    "These are the final nodes of a decision tree that provide the output label or prediction. Once data reaches a leaf node, the decision-making process ends.\n",
    "\n",
    "Branches (Edges)\n",
    "The branches represent the outcome of a decision and lead to the next node (which can be another decision node or a leaf node).\n",
    "\n"
   ],
   "id": "ecb41c64ba77d0db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How Decision Trees Work\n",
    "Splitting the Dataset\n",
    "At each node, the data is divided into subsets based on specific feature conditions. The goal is to create child nodes that are more homogeneous (i.e., contain more similar instances) than the parent node.\n",
    "Selecting the Best Splits\n",
    "Decision Trees use different algorithms to determine the best attribute and split point:\n",
    "Gini Impurity: Measures the frequency of incorrect classifications.\n",
    "Entropy and Information Gain: Measures the amount of information a split provides.\n",
    "The feature and threshold that provide the highest information gain or lowest impurity are selected.\n",
    "Recursive Partitioning\n",
    "The splitting process is applied recursively to each child node until one of the stopping criteria is met:\n",
    "All records in a node belong to the same class.\n",
    "There are no remaining features to split on.\n",
    "The maximum tree depth is reached.\n",
    "The node contains fewer records than the minimum sample split threshold.\n",
    "Tree Depth and Stopping Conditions\n",
    "Decision Trees can grow very deep, which may lead to overfitting. Stopping criteria and hyperparameters such as max_depth, min_samples_split, and min_samples_leaf help prevent this.\n",
    "Stopping conditions ensure:\n",
    "The model does not overfit by growing unnecessarily complex trees.\n",
    "The model maintains a balance between bias and variance."
   ],
   "id": "2a962834f59751a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Building a basic Decision Tree\n",
    "# Importing required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ],
   "id": "2498c017aef9da1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# preparing the data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Prepare data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ],
   "id": "446f1d0052a53d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predict and evaluate\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test) * 100)"
   ],
   "id": "f8f33654c0473250",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ],
   "id": "845b4350e681a037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cross-Validation to check for consistent accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n"
   ],
   "id": "b329faf9f1898ce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Decision Tree & Ensemble Workflow\n",
    "We will now be learning about common preprocessing techniques for Decision Trees and how to use them effectively."
   ],
   "id": "1953341e4af0d6e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Anatomy Of a Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define node positions (manually for simplicity)\n",
    "positions = {\n",
    "    'Root': (0.5, 1.0),\n",
    "    'Node_A': (0.25, 0.7),\n",
    "    'Node_B': (0.75, 0.7),\n",
    "    'Leaf_A1': (0.15, 0.4),\n",
    "    'Leaf_A2': (0.35, 0.4),\n",
    "    'Leaf_B1': (0.65, 0.4),\n",
    "    'Leaf_B2': (0.85, 0.4),\n",
    "}\n",
    "\n",
    "\n",
    "# Plot nodes\n",
    "def draw_node(name, pos, node_type):\n",
    "    color = 'white'\n",
    "    x, y = pos\n",
    "    if node_type == 'root':\n",
    "        color = 'red'\n",
    "    elif node_type == 'decision':\n",
    "        color = 'orange'\n",
    "    elif node_type == 'leaf':\n",
    "        color = 'green'\n",
    "\n",
    "    circle = plt.Circle((x, y), 0.05, color=color, ec='black', zorder=3)\n",
    "    plt.gca().add_patch(circle)\n",
    "    plt.text(x, y, name, ha='center', va='center', fontsize=10, zorder=4)\n",
    "\n",
    "\n",
    "# Draw edges/branches\n",
    "def draw_branch(start, end):\n",
    "    x1, y1 = positions[start]\n",
    "    x2, y2 = positions[end]\n",
    "    plt.plot([x1, x2], [y1, y2], 'k-', zorder=1)\n",
    "\n",
    "\n",
    "# Draw the nodes\n",
    "draw_node('Root Node', positions['Root'], 'root')\n",
    "draw_node('Decision\\nNode A', positions['Node_A'], 'decision')\n",
    "draw_node('Decision\\nNode B', positions['Node_B'], 'decision')\n",
    "draw_node('Leaf\\nPlay', positions['Leaf_A1'], 'leaf')\n",
    "draw_node('Leaf\\nNo Play', positions['Leaf_A2'], 'leaf')\n",
    "draw_node('Leaf\\nPlay', positions['Leaf_B1'], 'leaf')\n",
    "draw_node('Leaf\\nNo Play', positions['Leaf_B2'], 'leaf')\n",
    "\n",
    "# Draw branches\n",
    "draw_branch('Root', 'Node_A')\n",
    "draw_branch('Root', 'Node_B')\n",
    "draw_branch('Node_A', 'Leaf_A1')\n",
    "draw_branch('Node_A', 'Leaf_A2')\n",
    "draw_branch('Node_B', 'Leaf_B1')\n",
    "draw_branch('Node_B', 'Leaf_B2')\n",
    "\n",
    "# Annotations for labels\n",
    "plt.text(0.375, 0.85, 'Feature: Weather', ha='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.6))\n",
    "plt.text(0.1, 0.55, 'Sunny', ha='center', fontsize=8)\n",
    "plt.text(0.4, 0.55, 'Rainy', ha='center', fontsize=8)\n",
    "plt.text(0.6, 0.55, 'Overcast', ha='center', fontsize=8)\n",
    "plt.text(0.9, 0.55, 'Windy', ha='center', fontsize=8)\n",
    "\n",
    "# Aesthetic settings\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.axis('off')\n",
    "plt.title('Anatomy of a Decision Tree', fontsize=14)\n",
    "plt.show()\n"
   ],
   "id": "70855f72de85f237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 📚 Studying Gini Impurity and How Decision Trees Split\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 🟢 Load Iris dataset (classic ML dataset for classification)\n",
    "iris = load_iris()\n",
    "\n",
    "# 🟢 Convert to a DataFrame for easier manipulation and readability\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# 🟢 Add the target (species) as a new column\n",
    "iris_data['target'] = iris.target\n",
    "\n",
    "# 🟢 Optional: Map numeric targets to species names (human-readable)\n",
    "iris_data['species'] = iris_data['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# 🟢 Drop the numeric target column to avoid confusion (we'll use species names)\n",
    "iris_data = iris_data.drop(columns=['target'])\n",
    "\n",
    "# 🟢 Show the first few rows of the dataset\n",
    "iris_data.head()"
   ],
   "id": "43247b64a39e8c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Gini Impurity Calculation 🔷\n",
    "# This function calculates the Gini impurity for a set of labels.\n",
    "# Lower impurity means the node is more \"pure\" (one class dominates).\n",
    "def gini_impurity(labels):\n",
    "    total = len(labels)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    # Count how many times each label occurs\n",
    "    counts = labels.value_counts()\n",
    "    impurity = 1.0\n",
    "    # Subtract the squared probabilities from 1 to get impurity\n",
    "    for label in counts.index:\n",
    "        prob_of_label = counts[label] / total\n",
    "        impurity -= prob_of_label ** 2\n",
    "    return impurity"
   ],
   "id": "c88f2cf506220343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Split Data Function 🔷\n",
    "# This function splits the dataset into two groups:\n",
    "# Left split = rows where feature < value\n",
    "# Right split = rows where feature >= value\n",
    "def split_data(data, feature, value):\n",
    "    left_split = data[data[feature] < value]\n",
    "    right_split = data[data[feature] >= value]\n",
    "    return left_split, right_split"
   ],
   "id": "36f56b73c49638bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Information Gain Calculation 🔷\n",
    "# This function computes how much Gini impurity is reduced by making a split.\n",
    "# It helps us choose the best feature/value combination to split on.\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    # p = proportion of samples in the left split\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    # Subtract the weighted impurity of the left and right splits from current uncertainty\n",
    "    return current_uncertainty - p * gini_impurity(left['species']) - (1 - p) * gini_impurity(right['species'])\n"
   ],
   "id": "bdcd8a8549e99313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Find the Best Split 🔷\n",
    "# This function loops through all features and their unique values,\n",
    "# looking for the split that results in the highest information gain.\n",
    "def find_best_split(data):\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "    # Calculate the impurity of the current dataset (before splitting)\n",
    "    current_uncertainty = gini_impurity(data['species'])\n",
    "\n",
    "    # Exclude the target column (species) from features\n",
    "    n_features = len(data.columns) - 1\n",
    "\n",
    "    for feature in data.columns[:-1]:  # Skip the species column\n",
    "        values = data[feature].unique()  # All unique values of the feature\n",
    "        for val in values:\n",
    "            # Split data at this feature and value\n",
    "            left, right = split_data(data, feature, val)\n",
    "            # Ignore splits that don't divide the dataset\n",
    "            if len(left) == 0 or len(right) == 0:\n",
    "                continue\n",
    "            # Calculate information gain from this split\n",
    "            gain = info_gain(left, right, current_uncertainty)\n",
    "            # Update best gain/feature/value if this split is better\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_feature, best_value = gain, feature, val\n",
    "\n",
    "    return best_gain, best_feature, best_value"
   ],
   "id": "953172ea631e664f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Build the Tree (Recursive Partitioning) 🔷\n",
    "# This function builds the decision tree recursively.\n",
    "def build_tree(data):\n",
    "    gain, feature, value = find_best_split(data)\n",
    "\n",
    "    # Stopping condition: no information gain (pure node or no features left)\n",
    "    if gain == 0:\n",
    "        # Return the most common label (majority class) as a leaf\n",
    "        return {'Leaf': data['species'].mode()[0]}\n",
    "\n",
    "    # Split data into two groups based on the best split\n",
    "    left, right = split_data(data, feature, value)\n",
    "\n",
    "    # Recursively build the left and right branches of the tree\n",
    "    true_branch = build_tree(left)\n",
    "    false_branch = build_tree(right)\n",
    "\n",
    "    # Return a dictionary representing this node and its branches\n",
    "    return {\n",
    "        'Feature': feature,\n",
    "        'Value': value,\n",
    "        'TrueBranch': true_branch,\n",
    "        'FalseBranch': false_branch\n",
    "    }\n"
   ],
   "id": "949664ac6f996d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Build the Tree from the Iris Dataset 🔷\n",
    "tree = build_tree(iris_data)\n",
    "\n",
    "from pprint import pprint  # Pretty print the tree structure\n",
    "\n",
    "pprint(tree)\n"
   ],
   "id": "8c1b1fa9a77bd2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Classify a Single Sample 🔷\n",
    "# This function traverses the tree to make a prediction on a single sample.\n",
    "def classify(sample, node):\n",
    "    # If we reached a leaf, return the prediction\n",
    "    if 'Leaf' in node:\n",
    "        return node['Leaf']\n",
    "\n",
    "    # Check the feature's value for this sample\n",
    "    feature_val = sample[node['Feature']]\n",
    "\n",
    "    # Decide to go left (TrueBranch) or right (FalseBranch) based on the split value\n",
    "    if feature_val < node['Value']:\n",
    "        return classify(sample, node['TrueBranch'])\n",
    "    else:\n",
    "        return classify(sample, node['FalseBranch'])"
   ],
   "id": "63b18dc7c1a048a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Sample Predictions on Random Data Points 🔷\n",
    "# Take 5 random samples and predict their class\n",
    "samples = iris_data.sample(5)\n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    prediction = classify(row, tree)\n",
    "    actual = row['species']\n",
    "    print(f\"Predicted: {prediction}, Actual: {actual}\")\n",
    "\n"
   ],
   "id": "b5db541e7dc93eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Train/Test Split to Evaluate Accuracy 🔷\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into 70% training and 30% testing data\n",
    "train_data, test_data = train_test_split(iris_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the tree using only the training data\n",
    "tree = build_tree(train_data)\n",
    "\n",
    "# Test the tree by predicting the species of each sample in the test data\n",
    "correct = 0\n",
    "for idx, row in test_data.iterrows():\n",
    "    prediction = classify(row, tree)\n",
    "    actual = row['species']\n",
    "    print(f\"Predicted: {prediction}, Actual: {actual}\")\n",
    "    if prediction == actual:\n",
    "        correct += 1\n",
    "\n",
    "# Calculate and print the accuracy of the tree on test data\n",
    "accuracy = correct / len(test_data)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ],
   "id": "a140f9e419fec3fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 🔷 Visualize the Decision Tree 🔷\n",
    "\n",
    "\n",
    "def plot_tree(node, x=0.5, y=1.0, dx=0.7, dy=0.3, ax=None, depth=0, max_depth=None):\n",
    "    # First time setup\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(18, 10))  # Larger canvas helps!\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        # Calculate tree depth if not given\n",
    "        if max_depth is None:\n",
    "            max_depth = get_tree_depth(node)\n",
    "\n",
    "    # Leaf Node\n",
    "    if 'Leaf' in node:\n",
    "        ax.text(x, y, f\"Leaf:\\n{node['Leaf']}\",\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='darkgreen'),\n",
    "                fontsize=10)\n",
    "        return\n",
    "\n",
    "    # Decision Node\n",
    "    text = f\"{node['Feature']}\\n< {node['Value']:.2f}\"\n",
    "    ax.text(x, y, text,\n",
    "            ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='navy'),\n",
    "            fontsize=10)\n",
    "\n",
    "    # Dynamic horizontal spacing based on depth\n",
    "    spacing_factor = dx * (0.5 ** depth)\n",
    "    spacing_factor = max(spacing_factor, 0.03)  # Avoid too small spacing\n",
    "\n",
    "    # Left branch (True)\n",
    "    ax.plot([x, x - spacing_factor], [y - dy / 2, y - dy], 'k-')\n",
    "    plot_tree(node['TrueBranch'], x - spacing_factor, y - dy, dx, dy, ax, depth + 1, max_depth)\n",
    "\n",
    "    # Right branch (False)\n",
    "    ax.plot([x, x + spacing_factor], [y - dy / 2, y - dy], 'k-')\n",
    "    plot_tree(node['FalseBranch'], x + spacing_factor, y - dy, dx, dy, ax, depth + 1, max_depth)\n",
    "\n",
    "    # Show only at the top level recursion\n",
    "    if depth == 0:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_tree_depth(node):\n",
    "    \"\"\"Helper function to compute the max depth of a tree.\"\"\"\n",
    "    if 'Leaf' in node:\n",
    "        return 0\n",
    "    return 1 + max(get_tree_depth(node['TrueBranch']),\n",
    "                   get_tree_depth(node['FalseBranch']))\n",
    "\n",
    "\n",
    "plot_tree(tree)"
   ],
   "id": "51cd7ec6c5000ffa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Learning Pruning and using it on the adult income dataset\n",
    "\n",
    "# Importing required libraries\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset from UCI repository (or local CSV)\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "df = pd.read_csv(data_url, header=None, names=column_names, na_values=' ?')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Preprocessing\n",
    "label_encoders = {}\n",
    "\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']  # 1 = >50K, 0 = <=50K\n",
    "\n",
    "# Split into train and test sets (use lowercase variables for consistency)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Decision Tree without pruning as an example\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Accuracy (No Pruning):\", accuracy_score(y_test, y_pred))\n"
   ],
   "id": "20a6b84d48f5b3f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# adding pre-pruning\n",
    "# Pre-Pruned Decision Tree Classifier\n",
    "pre_pruned_clf = DecisionTreeClassifier(\n",
    "    max_depth=6,  # Limit tree depth\n",
    "    min_samples_split=10,  # Minimum samples required to split a node\n",
    "    min_samples_leaf=5,  # Minimum samples required at a leaf node\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "pre_pruned_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_pruned = pre_pruned_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Accuracy (Pre-Pruning):\", accuracy_score(y_test, y_pred_pruned))\n"
   ],
   "id": "c2576549e9dc15c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# post-pruning\n",
    "# Full decision tree (no restrictions)\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "ccp_alphas = path.ccp_alphas  # List of alpha values\n",
    "impurities = path.impurities  # Total impurity of leaves with corresponding alphas\n",
    "\n",
    "print(\"CCP Alphas:\", ccp_alphas)\n",
    "clfs = []  # A list to store each pruned tree\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, train_scores, marker='o', label='Train Accuracy', drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas, test_scores, marker='o', label='Test Accuracy', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs CCP Alpha\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "best_index = test_scores.index(max(test_scores))\n",
    "best_alpha = ccp_alphas[best_index]\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.5f}\")\n",
    "post_pruned_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
    "post_pruned_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_post_pruned = post_pruned_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy (Post-Pruned):\", accuracy_score(y_test, y_pred_post_pruned))\n",
    "print(classification_report(y_test, y_pred_post_pruned))\n"
   ],
   "id": "47937af34a859315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# handling class imbalance\n",
    "\n",
    "# option 1: balanced\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_balanced = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # <-- MAGIC\n",
    "    ccp_alpha=best_alpha  # use this if you're doing post-pruning\n",
    ")\n",
    "\n",
    "clf_balanced.fit(X_train, y_train)\n",
    "y_pred = clf_balanced.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Accuracy (with class_weight='balanced'):\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ],
   "id": "39954b107ebbab3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# option 2: SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Resample training data only!\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE:\", y_resampled.value_counts())\n",
    "clf_smote = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
    "clf_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = clf_smote.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (with SMOTE):\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "f761d5a620e81fc7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
