{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports, Loading data, and preprocessing\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# noinspection PyUnresolvedReferences\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "df = pd.read_csv(url, header=None, names=columns, na_values=' ?').dropna()\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Train-test split\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Smote\n",
    "def apply_smote(X_train, y_train, seed=42):\n",
    "    sm = SMOTE(random_state=seed)\n",
    "    return sm.fit_resample(X_train, y_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Training and Evaluation Function\n",
    "def train_and_evaluate(model, name, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n🧠 {name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n"
   ],
   "id": "c814012fbe11d635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Import required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# noinspection PyUnresolvedReferences - this is still required; scikit-learn a metrics common problem\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# random forest\n",
    "X_rf, y_rf = apply_smote(X_train, y_train, seed=1)\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=15, random_state=42)\n",
    "train_and_evaluate(rf, \"Random Forest\", X_rf, y_rf)\n"
   ],
   "id": "dd4870bc9a884e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# XGBoost\n",
    "X_xgb, y_xgb = apply_smote(X_train, y_train, seed=2)\n",
    "xgb = XGBClassifier(eval_metric='logloss', n_estimators=150, max_depth=7, random_state=42)\n",
    "train_and_evaluate(xgb, \"XGBoost\", X_xgb, y_xgb)"
   ],
   "id": "1890837d17e83ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gradient Boosting Classifier\n",
    "X_gbc, y_gbc = apply_smote(X_train, y_train, seed=3)\n",
    "gbc = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "train_and_evaluate(gbc, \"Gradient Boosting\", X_gbc, y_gbc)"
   ],
   "id": "23b7c2ba7fb7d1c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LightGBM\n",
    "X_lgb, y_lgb = apply_smote(X_train, y_train, seed=4)\n",
    "lgbm = LGBMClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "train_and_evaluate(lgbm, \"LightGBM\", X_lgb, y_lgb)\n"
   ],
   "id": "4a1049f60e9c8388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using optuna for hyperparameter tuning (slow and often does not get you the best ranges, but if you're willing to sacrifice a lot of time it could help.)\n",
    "# Make sure optuna is installed\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 600)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.5)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        subsample=subsample,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Use cross-validation for stability\n",
    "    score = cross_val_score(model, X_resampled, y_resampled, cv=3, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=30) # Try 30 different combos - 15 is the best combo\n",
    "study.optimize(objective, n_trials=18) # A more practical take\n",
    "\n",
    "# Show the best result\n",
    "print(\"✅ Best Parameters:\", study.best_params)\n",
    "\n"
   ],
   "id": "8fcf310106982674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 📦 Import stacking-related modules\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# noinspection PyUnresolvedReferences\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ✅ Define base models (level-0)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "    ('lgb', LGBMClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# ✅ Define meta-model (level-1)\n",
    "# You can use LogisticRegression for speed or GradientBoosting for power\n",
    "meta_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# ✅ Build Stacking Classifier\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "#  FixMe: Cpt!\n",
    "# ✅ Fit the stack on SMOTE-resampled data - THIS IS A VERSION CONFLICT - INVALID\n",
    "# stack.fit(X_resampled, y_resampled)\n",
    "\n",
    "# ✅ Evaluate on untouched test data\n",
    "# y_pred_stack = stack.predict(X_test)\n",
    "# print(\"📊 Stacked Accuracy:\", accuracy_score(y_test, y_pred_stack))\n",
    "# print(classification_report(y_test, y_pred_stack))\n",
    "\n"
   ],
   "id": "aabeceaf79800d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Using SHAP\n",
    "import shap\n",
    "\n",
    "\n",
    "# ✅ Initialize the model\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Tree-based models only!\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot summary\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ],
   "id": "5a0257314c5bcb26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# threshold tuning\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "\n",
    "# Find best F1 threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "print(\"🔍 Best Threshold:\", best_threshold)\n",
    "\n"
   ],
   "id": "6964b8d401405472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use the best threshold to make final predictions\n",
    "final_preds = (probs > 0.428).astype(int)\n",
    "\n",
    "# Evaluate again\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"✅ Accuracy (Custom Threshold):\", accuracy_score(y_test, final_preds))\n",
    "print(classification_report(y_test, final_preds))\n",
    "print(confusion_matrix(y_test, final_preds))\n"
   ],
   "id": "30ecf299b28435d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# shap dropping unimportant features\n",
    "import shap\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train model\n",
    "model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Get SHAP values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Plot feature importance\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n"
   ],
   "id": "d2517cf1081a7986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean absolute SHAP value for each feature\n",
    "shap_abs_mean = np.abs(shap_values).mean(axis=0)\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': shap_abs_mean\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(shap_importance)\n"
   ],
   "id": "e30b9b95c89fa1c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keep top N most important features\n",
    "top_k = 10  # choose how many you want\n",
    "top_features = shap_importance['feature'].head(top_k).tolist()\n",
    "\n",
    "# Reduce X accordingly\n",
    "X_top = X[top_features]\n"
   ],
   "id": "91e0fce11e521c42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train again\n",
    "model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "y_preds = (y_probs > 0.428).astype(int)  # use your threshold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy (retrained):\", accuracy_score(y_test, y_preds))\n",
    "print(classification_report(y_test, y_preds))\n"
   ],
   "id": "b0d1ab414c870121",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Rdd13r grade is A, not A+ - version-specific changes required.",
   "id": "c8981bd1a538e5b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
