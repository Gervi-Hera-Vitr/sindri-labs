{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewmvd/heart-failure-clinical-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Explanation\n",
    "In this project, I’m leveraging advanced ensemble methods, specifically boosting algorithms like CatBoost, XGBoost, and LightGBM, combined into a stacking ensemble to predict heart failure outcomes with high accuracy. Boosting algorithms are particularly effective because they iteratively focus on the data points that are hardest to predict, which helps reduce errors and create a model that generalizes well. By combining these individual models in a stacking ensemble, I’m taking advantage of their unique strengths—CatBoost’s ability to handle categorical data, XGBoost’s speed and efficiency, and LightGBM’s capacity to handle large datasets—resulting in a model that’s not just accurate but also robust. To address the class imbalance in the data (e.g., fewer instances of heart failure), I used SMOTE, a synthetic oversampling technique, to balance the dataset and ensure the model treats both classes fairly. This approach works so well because it captures complex patterns in the data while mitigating overfitting, ultimately achieving a high accuracy of 92.68% and a stellar ROC-AUC of 98%, making it both predictive and reliable for real-world applications."
   ],
   "id": "121973627c767b68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = f\"{path}/heart_failure_clinical_records_dataset.csv\"  # Replace with actual filename if different\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n"
   ],
   "id": "2ee7e70a2339ad7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "# Assuming 'data' is already loaded as a pandas DataFrame\n",
    "# Replace 'data.csv' with the path to your dataset\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['DEATH_EVENT'])\n",
    "y = data['DEATH_EVENT']\n",
    "\n",
    "# Feature selection: Drop low-importance features\n",
    "X = X.drop(columns=['smoking', 'anaemia', 'high_blood_pressure'])\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Initialize base models\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=500, learning_rate=0.1, depth=4, l2_leaf_reg=3, random_seed=42, verbose=0\n",
    ")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500, learning_rate=0.1, max_depth=4, reg_lambda=3, random_state=42\n",
    ")\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=500, learning_rate=0.1, max_depth=4, reg_lambda=3, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Stacking Classifier\n",
    "estimators = [\n",
    "    ('catboost', catboost_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgbm', lgbm_model),\n",
    "]\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=CatBoostClassifier(\n",
    "        iterations=200, learning_rate=0.05, depth=6, random_seed=42, verbose=0\n",
    "    ), cv=3\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "637b71c03e8bf748",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
