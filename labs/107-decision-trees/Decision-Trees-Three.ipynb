{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# training a simple tree on a noisy sine curve.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate sample data\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Fit a regression model\n",
    "regressor = DecisionTreeRegressor(max_depth=3)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", label=\"data\")\n",
    "plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# multi-output regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Input data: x values\n",
    "X = np.sort(np.random.rand(100, 1) * 6, axis=0)\n",
    "\n",
    "# Multi-output: y is both sin(x) and cos(x)\n",
    "y = np.hstack((np.sin(X), np.cos(X)))  # shape: (100, 2)\n",
    "\n",
    "# Fit a multi-output regressor\n",
    "multi_reg = DecisionTreeRegressor(max_depth=4)\n",
    "multi_reg.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "y_pred = multi_reg.predict(X_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X_test, y_pred[:, 0], label='sin(x) prediction')\n",
    "plt.plot(X_test, y_pred[:, 1], label='cos(x) prediction')\n",
    "plt.xlabel(\"x\")\n",
    "plt.title(\"Multi-output Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "d95bc373af581ca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# bagging decision trees\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    max_samples=0.8,\n",
    "    bootstrap=True,  # with replacement\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n"
   ],
   "id": "61ef13d7f493169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use histboost and adaboost\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the wine dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale for AdaBoost (HistGradientBoosting doesn't need it)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "hgb.fit(X_train, y_train)\n",
    "print(\"HistGradientBoosting Accuracy:\", round(hgb.score(X_test, y_test), 3))\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "print(\"AdaBoostClassifier Accuracy:\", round(ada.score(X_test_scaled, y_test), 3))\n"
   ],
   "id": "f8e6aa553d15fb7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset from UCI repo (or local CSV)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "df = pd.read_csv(url, header=None, names=columns, na_values=' ?')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Binary target\n",
    "df['income'] = df['income'].apply(lambda x: 1 if '>50K' in x else 0)\n",
    "\n",
    "# Split data\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "])\n",
    "\n",
    "# Base models\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf2 = RandomForestClassifier(n_estimators=100)\n",
    "clf3 = SVC(probability=True)\n",
    "\n",
    "# Ensemble with voting\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Pipeline with preprocessing + voting model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', voting_clf)\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"VotingClassifier Accuracy on Adult Dataset: {acc:.3f}\")\n"
   ],
   "id": "8ddcf3e2a6908f7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# multiclass classification\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "model = OneVsRestClassifier(LinearSVC(max_iter=5000)).fit(X, y)\n",
    "print(model.score(X, y))\n"
   ],
   "id": "cbbe307d704ac218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# multilable classification\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples=100, n_classes=5, random_state=0)\n",
    "model = MultiOutputClassifier(RandomForestClassifier()).fit(X, y)\n",
    "print(model.predict(X[:5]))\n"
   ],
   "id": "f6a78b04b15b9173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# multiclass-multioutput classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X, y1 = make_classification(n_samples=100, n_classes=3, n_informative=10, random_state=0)\n",
    "y2 = shuffle(y1, random_state=1)\n",
    "y3 = shuffle(y1, random_state=2)\n",
    "Y = np.vstack((y1, y2, y3)).T\n",
    "\n",
    "model = MultiOutputClassifier(RandomForestClassifier()).fit(X, Y)\n",
    "print(model.predict(X[:5]))\n"
   ],
   "id": "f348b1408509592e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## multioutput regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_targets=3, noise=0.1, random_state=0)\n",
    "model = MultiOutputRegressor(GradientBoostingRegressor()).fit(X, y)\n",
    "print(model.predict(X[:5]))\n"
   ],
   "id": "d8d3c74d996ef979",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using a nural network to solve the adult dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load Adult dataset\n",
    "X, y = fetch_openml(\"adult\", version=2, as_frame=True, return_X_y=True)\n",
    "X = pd.get_dummies(X)  # One-hot encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), solver=\"adam\", max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MLPClassifier Accuracy on Adult dataset: {accuracy:.4f}\")\n"
   ],
   "id": "e99867a573a55139",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
